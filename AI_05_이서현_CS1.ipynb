{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_05_이서현_CP1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1V7nF3ZG68bAvwPFeJgL79IK_tEQd1QfD",
      "authorship_tag": "ABX9TyP5A1bVF/ntt9qYzcDIUBxl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eeseohyun/project/blob/main/AI_05_%EC%9D%B4%EC%84%9C%ED%98%84_CP1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**라이브러리 불러오기**"
      ],
      "metadata": {
        "id": "E7cRNBaPeIvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import keras\n",
        "\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout,BatchNormalization,MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "T0AQISmPeL6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With_Mask**"
      ],
      "metadata": {
        "id": "ZYR2zTlAf-i4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "c2ptMJMsjujs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_withmask = os.listdir('/content/drive/MyDrive/Colab Notebooks/New Masks Dataset/Train/Mask')\n",
        "sample_img_withmask = random.choice(images_withmask)\n",
        "image = load_img(\"/content/drive/MyDrive/Colab Notebooks/New Masks Dataset/Train/Mask/\"+sample_img_withmask)\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "KiPHqGo1gNw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Without_Mask**"
      ],
      "metadata": {
        "id": "1R9FCx0CmFMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images_withoutmask = os.listdir('/content/drive/MyDrive/Colab Notebooks/New Masks Dataset/Train/Non Mask')\n",
        "sample_img_withoutmask = random.choice(images_withoutmask)\n",
        "image = load_img(\"/content/drive/MyDrive/Colab Notebooks/New Masks Dataset/Train/Non Mask/\"+sample_img_withoutmask)\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "tWvASM6Vktli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Augmentation (데이터 증식)**"
      ],
      "metadata": {
        "id": "zKgOcUy6nNmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 너비&높이 정의\n",
        "img_width = 224\n",
        "img_height = 224\n",
        "\n",
        "# 초기 학습률, 훈련할 Epoch 수 및 배치 크기 초기화\n",
        "LearningRate = 1e-4\n",
        "EPOCHS = 50\n",
        "BatchSize = 32\n",
        "train_dir = \"/content/drive/MyDrive/Colab Notebooks/New Masks Dataset/Train\"\n",
        "test_dir = \"/content/drive/MyDrive/Colab Notebooks/New Masks Dataset/Validation\""
      ],
      "metadata": {
        "id": "0pQ_2CmbmgGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_imagepaths = list(paths.list_images(train_dir))\n",
        "all_test_imagepaths = list(paths.list_images(test_dir))\n",
        "train_data = []\n",
        "train_labels = []\n",
        "test_data = []\n",
        "test_labels = []"
      ],
      "metadata": {
        "id": "gmNbsFoXoH8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train 이미지 경로에 대한 루프\n",
        "for imagePath in all_train_imagepaths:\n",
        "  label = imagePath.split(os.path.sep)[-2]     # 파일명에서 클래스 레이블 추출\n",
        "  \n",
        "  image = load_img(imagePath, target_size=(224,224))     # 입력 이미지(224x224)를 로드하고 전처리\n",
        "  image = img_to_array(image)\n",
        "  image = preprocess_input(image)    # 이미지 배치를 인코딩하는 텐서 또는 Numpy 배열을 전처리\n",
        "  \n",
        "  # 데이터 및 레이블을 각각 업데이트\n",
        "  train_data.append(image)\n",
        "  train_labels.append(label)"
      ],
      "metadata": {
        "id": "TAvNr5KnogDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test 이미지 경로에 대한 루프\n",
        "for imagePath in all_test_imagepaths:\n",
        "\t# extract the class label from the filename\n",
        "\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "\t# load the input image (224x224) and preprocess it\n",
        "\timage = load_img(imagePath, target_size=(224, 224))\n",
        "\n",
        "\timage = img_to_array(image)\n",
        "\timage = preprocess_input(image)\n",
        "\n",
        "\t# update the data and labels lists, respectively\n",
        "\ttest_data.append(image)\n",
        "\ttest_labels.append(label)"
      ],
      "metadata": {
        "id": "CS8FUHoh_7Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_data_labels(data,labels):\n",
        "  # 데이터와 레이블을 NumPy 배열로 변환\n",
        "  data = np.array(data, dtype=\"float32\")\n",
        "  labels = np.array(labels)\n",
        "  \n",
        "  # 라벨 이진화\n",
        "  lb = LabelBinarizer()\n",
        "  labels = lb.fit_transform(labels)\n",
        "  # 라벨 원핫 인코딩\n",
        "  labels = to_categorical(labels)\n",
        "  return  data,labels"
      ],
      "metadata": {
        "id": "ridjcGJzpzTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data,train_labels = convert_data_labels(train_data,train_labels)\n",
        "test_data,test_labels = convert_data_labels(test_data,test_labels)"
      ],
      "metadata": {
        "id": "TQcCcnFNqsST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of train dataset : \",train_data.shape[0])\n",
        "print(\"Size of test dataset : \",test_data.shape[0])"
      ],
      "metadata": {
        "id": "6DCHk78zq4mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ImageDataGenerator를 이용한 데이터 증식\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range = 45,     # 회전에 대한 각도 범위\n",
        "    zoom_range = 0.15,       # [아래, 위] 임의 확대/축소 범위\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    shear_range = 0.15,      # 반시계 방향의 전단 각도\n",
        "    horizontal_flip = True,  # 무작위로 입력을 수평으로 뒤집기\n",
        "    fill_mode = 'nearest')   #'constant': kkkkkkkk|abcd|kkkkkkkk(cval=k)\n",
        "                             #'nearest': aaaaaaaa|abcd|dddddddd\n",
        "                             #'reflect': abcddcba|abcd|dcbaabcd\n",
        "                             #'warp': abcdabcd|abcd|abcdabcd\n",
        "\n",
        "train_generator = train_datagen.flow(train_data, train_labels, batch_size=BatchSize)"
      ],
      "metadata": {
        "id": "hOHl7QjbAkAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modeling**\n",
        "\n",
        "- MobileNetV2"
      ],
      "metadata": {
        "id": "aBu-IE6FD6xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mobilenet = MobileNetV2(weights=\"imagenet\", # ImageNet에 대한 사전 학습\n",
        "                        include_top=False,  # 네트워크 상단에 완전 연결 계층x\n",
        "                        input_tensor=Input(shape=(224, 224, 3))) # 3개의 입력 채널"
      ],
      "metadata": {
        "id": "xWNUmWMGDxl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 모델을 기반으로 head모델 구성\n",
        "headModel = mobilenet.output\n",
        "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(2, activation=\"softmax\")(headModel)"
      ],
      "metadata": {
        "id": "rvEvvxQ2EM3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# head모델을 기본 모델 위에 배치(= 실제 훈련할 모델)\n",
        "model = Model(inputs=mobilenet.input, outputs=headModel)"
      ],
      "metadata": {
        "id": "-dXbSwmtEhxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile Model\n",
        "opt = Adam(learning_rate=LearningRate, decay=LearningRate / EPOCHS)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "sztONGVRE3W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫 훈련과정 중 업데이트되지 않기 위해 기본 모델의 모든 레이어를 반복하고 고정\n",
        "for layer in mobilenet.layers:\n",
        "\tlayer.trainable = False"
      ],
      "metadata": {
        "id": "j7R6uq4VFIXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Callback Function**"
      ],
      "metadata": {
        "id": "BXSlgbqWFu6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')\n",
        "# # history=model.fit(train_data,train_target,epochs=100,callbacks=[checkpoint],validation_split=0.2)\n",
        "# EarlyStopping\n",
        "earlystop = EarlyStopping(monitor = 'val_loss',\n",
        "                          min_delta = 0,\n",
        "                          patience = 7,\n",
        "                          verbose = 1,\n",
        "                          restore_best_weights = True)\n",
        "\n",
        "# ModelCheckPoint\n",
        "checkPoint = keras.callbacks.ModelCheckpoint(filepath=\"/content/sample_data/fmd_model.h5\",\n",
        "                             monitor='val_loss',\n",
        "                             mode='auto',\n",
        "                             save_best_only=True,\n",
        "                             verbose=1)\n",
        "\n",
        "# ReduceLROnPlateau\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
        "                                            patience=2, \n",
        "                                            verbose=1, \n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.00001)"
      ],
      "metadata": {
        "id": "iD0-gYj-FuGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [earlystop , checkPoint, learning_rate_reduction]"
      ],
      "metadata": {
        "id": "aVTxA6_BF3jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Fitting**"
      ],
      "metadata": {
        "id": "HmtBbx59F6xH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = model.fit(\n",
        "    train_datagen.flow(train_data, train_labels, batch_size=BatchSize), \n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(test_data,test_labels),\n",
        "    validation_steps=len(test_data)//BatchSize,\n",
        "    steps_per_epoch=len(train_data)//BatchSize,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "yWEGKEibF6KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loss and Accuracy Visualization**"
      ],
      "metadata": {
        "id": "ByceRChgMJRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('ggplot')\n",
        "N = 10\n",
        "plt.figure()\n",
        "plt.plot( classifier.history[\"loss\"], label='train_loss')\n",
        "plt.plot( classifier.history['val_loss'], label='val_loss')\n",
        "plt.plot( classifier.history['accuracy'], label='train_acc')\n",
        "plt.plot( classifier.history['val_accuracy'], label='val_acc')\n",
        "plt.title('Training Loss and Accuracy')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc='center right')"
      ],
      "metadata": {
        "id": "CFEkNVMsMKyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Find Accuracy**"
      ],
      "metadata": {
        "id": "fxoa8N80Ndqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss,val_acc = model.evaluate(test_data,test_labels)\n",
        "\n",
        "print(\"Accuracy: \",val_acc)\n",
        "print(\"=======================================================\")\n",
        "print(\"Loss: \",val_loss)"
      ],
      "metadata": {
        "id": "dfOA9wcYNdBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**라이브러리 불러오기**"
      ],
      "metadata": {
        "id": "txwSQroaOHi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from imutils.video import VideoStream\n",
        "import argparse\n",
        "import imutils\n",
        "import time"
      ],
      "metadata": {
        "id": "CEuzjS8sORJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**알람 소리**"
      ],
      "metadata": {
        "id": "DWUjEwywOjyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygame"
      ],
      "metadata": {
        "id": "-u-BekZ9OgAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pygame\n",
        "pygame.init()\n",
        "sound = pygame.mixer.Sound('/content/drive/MyDrive/Colab Notebooks/mixkit-sound-alert-in-hall-1006.wav')"
      ],
      "metadata": {
        "id": "3mhcrO-53HW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image Pre-Processing**"
      ],
      "metadata": {
        "id": "iV63U4E1OwPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_detection_prediction(frame, faceNet, maskNet):\n",
        "  # 프레임 차원을 찾고 blob 구성\n",
        "  (h, w) = frame.shape[:2]\n",
        "  blob = cv2.dnn.blobFromImage(frame, 1.0, (224, 224),(104.0, 177.0, 123.0))\n",
        "\n",
        "  # 네트워크를 통해 blob을 전달하고 얼굴 감지\n",
        "  faceNet.setInput(blob)\n",
        "  detections = faceNet.forward()\n",
        "\n",
        "  # 얼굴, 얼굴 위치 및 예측 목록을 저장할 빈 목록 생성\n",
        "  faces = []\n",
        "  locs = []\n",
        "  preds = []\n",
        "\n",
        "  # loop\n",
        "  for i in range(0, detections.shape[2]):\n",
        "    # 탐지와 관련된 신뢰도 또는 확률 찾기\n",
        "    confidence = detections[0, 0, i, 2]\n",
        "\n",
        "    # 강력한 탐지 필터링\n",
        "    if confidence > 0.5:\n",
        "\n",
        "        # bounding box의 시작, 끝 좌표 찾기\n",
        "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "        # bounding box  가 프레임 크기 내에 있는지 확인\n",
        "        (startX, startY) = (max(0, startX), max(0, startY))\n",
        "        (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        "\n",
        "        # 얼굴 ROI 추출, BGR에서 RGB 채널로 변환\n",
        "        # ordering, (224x224) 사이즈 조정, 전처리\n",
        "        face = frame[startY:endY, startX:endX]\n",
        "        face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "        face = cv2.resize(face, (224, 224))\n",
        "        face = img_to_array(face)\n",
        "        face = preprocess_input(face)\n",
        "\n",
        "        # 얼굴 및 bounding box를 해당 목록에 추가\n",
        "        faces.append(face)\n",
        "        locs.append((startX, startY, endX, endY))\n",
        "\n",
        "    # 적어도 하나의 얼굴이 감지된 경우에만 예측\n",
        "    if len(faces) > 0:\n",
        "        # 더 빠른 추론을 위해 위의 'for' 루프에서 하나씩 예측하는 대신 모든 얼굴에 대해 동시에 일괄 예측을 수행\n",
        "        faces = np.array(faces, dtype=\"float32\")\n",
        "        preds = maskNet.predict(faces, batch_size=32)\n",
        "\n",
        "    # 얼굴 위치와 해당 예측의 2튜플을 반환\n",
        "    return (locs, preds)"
      ],
      "metadata": {
        "id": "6Ep5rYr7OyEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Load Caffe Model**\n",
        "\n",
        "- **Caffe**(Convolutional Architecture for Fast Feature Embedding)\n",
        "\n",
        "  : 사용자가 이미지 분류 및 이미지 분할 모델을 생성할 수 있도록 하는 딥러닝 프레임워크\n",
        "\n"
      ],
      "metadata": {
        "id": "rHAA91sFc2v0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os.path import dirname, join\n",
        "\n",
        "prototxtPath = join(\"face_detector\", \"deploy.prototxt\")\n",
        "weightsPath = join(\"face_detector\", \"res10_300x300_ssd_iter_140000.caffemodel\")\n",
        "\n",
        "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
        "\n",
        "# 디스크에서 안면 마스크 감지기 모델 로드\n",
        "maskNet = load_model(\"fmd_model.h5\")"
      ],
      "metadata": {
        "id": "HqWS0qXOdA7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Face Detection on Live Camera**"
      ],
      "metadata": {
        "id": "6nXQX6AHdaR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the video stream\n",
        "vs = VideoStream(src=0).start()\n",
        "\n",
        "# VideoStream의 frame 반복\n",
        "while True:\n",
        "    \n",
        "    # 스레드된 비디오 스트림에서 프레임을 잡고 너비가 400픽셀이 되도록 크기 조정\n",
        "    frame = vs.read()\n",
        "    frame = imutils.resize(frame, width=400)\n",
        "\n",
        "    # 프레임에서 얼굴을 감지하고 마스크 착용여부 확인\n",
        "    (locs, preds) = mask_detection_prediction(frame, faceNet, maskNet)\n",
        "\n",
        "    # 감지된 얼굴 위치와 해당 위치에 대한 루프\n",
        "    for (box, pred) in zip(locs, preds):\n",
        "        # bounding box 및 예측 압축 풀기\n",
        "        (startX, startY, endX, endY) = box\n",
        "        (mask, withoutMask) = pred\n",
        "\n",
        "        if mask>withoutMask:\n",
        "            label = \"Mask\"\n",
        "            color = (0, 255, 0)\n",
        "\n",
        "            print(\"Normal\")\n",
        "        else:\n",
        "            label = \"No Mask\"\n",
        "            color = (0, 0, 255)\n",
        "            sound.play()\n",
        "\n",
        "            print(\"Alert!!!\")\n",
        "\n",
        "        # 레이블에 확률 포함\n",
        "        label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\n",
        "        # 출력 프레임에 레이블 및 경계 상자에 사각형 표시\n",
        "        cv2.putText(frame, label, (startX, startY - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "        cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "    # 출력 프레임 표시\n",
        "    cv2.imshow(\"Frame\", frame)\n",
        "    key = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "    # `q` 키가 눌렸다면 루프 정지\n",
        "    if key == ord(\"q\"):\n",
        "        break\n",
        "\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "vs.stop()"
      ],
      "metadata": {
        "id": "L59dIMm9dbmO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
